{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pedestrians recognition without using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import glob\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "\n",
    "data_root = \"img1\"\n",
    "BACK_SUB_FOLDER_PATH = \"back_sub\"\n",
    "MAX_DIST_CONSIST = 8\n",
    "N_CONSISTENCY = 3\n",
    "\n",
    "gt_path = './gt/gt.txt'\n",
    "\n",
    "_W = 1280\n",
    "_H = 960\n",
    "_N = 684 # number of frames\n",
    "\n",
    "WIDTH_SAMPLE_SVM = 15\n",
    "HEIGHT_SAMPLE_SVM = 30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and dataset function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_id(frame):\n",
    "    assert _N >= frame\n",
    "    return '{:03d}'.format(frame)\n",
    "\n",
    "\n",
    "def read_frame(root, frame):\n",
    "    \"\"\"Read frames and create integer frame_id-s\"\"\"\n",
    "    assert _N >= frame\n",
    "    return cv2.imread(os.path.join(root,format_id(frame)+'.jpg'), cv2.IMREAD_UNCHANGED)\n",
    "    \n",
    "\n",
    "def read_gt(filename):\n",
    "    \"\"\"Read gt and create list of bb-s\"\"\"\n",
    "    assert os.path.exists(filename)\n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    # truncate data (last columns are not needed)\n",
    "    return [list(map(lambda x: int(x), line.split(',')[:6])) for line in lines]\n",
    "\n",
    "\n",
    "def annotations_for_frame(solution, frame):\n",
    "    assert _N >= frame\n",
    "    return [bb for bb in solution if int(bb[0])==int(frame)]\n",
    "\n",
    "\n",
    "def evaluate_solution(gt, solution, N):\n",
    "    \"\"\"Caclulate evaluation metric\"\"\"\n",
    "    score = []\n",
    "    #for frame in [300]:\n",
    "    for frame in range(1, N):\n",
    "        bbs_sol = annotations_for_frame(solution, frame)\n",
    "        bbs_gt = annotations_for_frame(gt, frame)\n",
    "        black_sol = np.zeros((_H, _W))\n",
    "        black_gt = np.zeros((_H, _W))\n",
    "        for bb in bbs_sol:\n",
    "            x, y = bb[2:4]\n",
    "            dx, dy = bb[4:6]\n",
    "            cv2.rectangle(black_sol, (x, y), (x+dx, y+dy), (255), -1)\n",
    "        for bb in bbs_gt:\n",
    "            x, y = bb[2:4]\n",
    "            dx, dy = bb[4:6]\n",
    "            cv2.rectangle(black_gt, (x, y), (x+dx, y+dy), (255), -1)\n",
    "        # intersection over union\n",
    "        intersection = black_sol * black_gt\n",
    "        intersection[intersection > 0.5] = 1\n",
    "        union = black_sol + black_gt\n",
    "        union[union > 0.5] = 1\n",
    "        if not union.any():\n",
    "            continue\n",
    "        score.append(intersection.sum()/union.sum())\n",
    "        \n",
    "        \n",
    "    return np.asarray(score).mean()\n",
    "    \n",
    "\n",
    "def show_annotation(solution, frame):\n",
    "    assert _N >= frame\n",
    "    im = read_frame(data_root, frame)\n",
    "    bbs = annotations_for_frame(solution, frame)\n",
    "    for bb in bbs:\n",
    "        x, y = bb[2:4]\n",
    "        dx, dy = bb[4:6]\n",
    "        cv2.rectangle(im, (x, y), (x+dx, y+dy), (0,255,0), 10)\n",
    "    plt.imshow(im)\n",
    "    plt.title('Annotations for frame {}.'.format(frame))\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOG function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hog(im_path):\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    im = cv2.imread(im_path)\n",
    "    h = hog.compute(im)\n",
    "    print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disp_image(im):\n",
    "    imgplot = plt.imshow(im)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background substraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def background_substraction(les_im_path, video_name=None, back_sub_folder=\"back_sub\"):\n",
    "    \"\"\"\n",
    "    Generate a video with background substraction\n",
    "    :param les_im_path: list of string, images path\n",
    "    :param video_name: string, default = None, if set, will save a video of the backgroud substraction to 'video_name'.avi\n",
    "    :param back_sub_folder: string, default = 'back_sub', folder used to save background substracted images\n",
    "    \n",
    "    OUTPUT:\n",
    "        back_sub/XXX.jpg : images of the background substraction\n",
    "        \n",
    "        if video_name != None:\n",
    "            video_name.avi : video of the background substraction\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create background frame\n",
    "    fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "    \n",
    "    # Create output folder if doesn't already exist\n",
    "    try:\n",
    "        os.mkdir(back_sub_folder)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    if video_name is not None:\n",
    "        # Get video size\n",
    "        height,width,layers=cv2.imread(les_im_path[0]).shape\n",
    "\n",
    "        # Create video object\n",
    "        fourcc = cv2.VideoWriter_fourcc('M','J','P','G')\n",
    "        video = cv2.VideoWriter(video_name + '.avi', fourcc, fps=25, frameSize=(width,height), isColor=False)\n",
    "    \n",
    "    # Computing background\n",
    "    for im_path in les_im_path:\n",
    "        im = cv2.imread(im_path)\n",
    "        fgmask = fgbg.apply(im)\n",
    "        \n",
    "        cv2.imwrite(back_sub_folder + '/' + im_path.split(\"/\")[-1], fgmask)\n",
    "        \n",
    "        if video_name is not None:\n",
    "            # Writing frame to video\n",
    "            video.write(fgmask)\n",
    "    \n",
    "    if video_name is not None:\n",
    "        # Close video\n",
    "        video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get human shaped contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Box:\n",
    "    def __init__(self, x_min, y_min, x_max, y_max):\n",
    "        self.x_min = x_min\n",
    "        self.y_min = y_min\n",
    "        self.x_max = x_max\n",
    "        self.y_max = y_max\n",
    "    \n",
    "    def __repr__(self):\n",
    "        ret = \"x_min\" + str(self.x_min) + \", y_min\" + str(self.y_min) + \", x_max\" + str(self.x_max) + \", y_max\" + str(self.y_max)\n",
    "        return ret\n",
    "    \n",
    "    def is_overlap(self, other):\n",
    "        return not (self.x_max < other.x_min or self.x_min > other.x_max or self.y_max < other.y_min or self.y_min > other.y_max)\n",
    "\n",
    "        \n",
    "def filter_ratio(box, ratio_min, ratio_max, disp=False):\n",
    "    width = box.x_max - box.x_min\n",
    "    height = box.y_max - box.y_min\n",
    "\n",
    "    # ratio\n",
    "    ratio = height / width\n",
    "\n",
    "    if ratio > ratio_min and ratio < ratio_max:\n",
    "        return box\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "        \n",
    "def get_contours_human_ratio(im_path, disp=False):\n",
    "    # Load image and resize witout changing ratio\n",
    "    im = cv2.imread(im_path)\n",
    "    height, width = im.shape[:2]\n",
    "    new_width = 500\n",
    "    new_height = new_width*height//width\n",
    "    im = cv2.resize(im,(new_width, new_height), interpolation = cv2.INTER_CUBIC)\n",
    "    \n",
    "    # Change to gray and apply both gaussian and threshold filter\n",
    "    im_gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "    blurred_im = cv2.GaussianBlur(im_gray, (1, 1), 0)\n",
    "    ret,thresh = cv2.threshold(blurred_im, 220, 255, 0)\n",
    "    \n",
    "    # Compute contours\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Get dimension of main contours\n",
    "    les_potential_human_box = []\n",
    "    for cnt in contours:\n",
    "        # Compute area size\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if area > 3:\n",
    "            # remove overdimension of contours\n",
    "            cnt_low = cnt[:, 0]\n",
    "            \n",
    "            if disp:\n",
    "                print(area)\n",
    "                im2 = cv2.drawContours(im.copy(), cnt, -1, (255,0,0), 2)\n",
    "                disp_image(im2)\n",
    "    \n",
    "            # contour width\n",
    "            x_max = np.max(cnt_low[:, 0])*width//new_width\n",
    "            x_min = np.min(cnt_low[:, 0])*width//new_width\n",
    "            # contour height\n",
    "            y_max = np.max(cnt_low[:, 1])*height//new_height\n",
    "            y_min = np.min(cnt_low[:, 1])*height//new_height\n",
    "            \n",
    "            potential_human_box = filter_ratio(Box(x_min, y_min, x_max, y_max), 1, 5)\n",
    "            if potential_human_box is not None:\n",
    "                les_potential_human_box.append(potential_human_box)\n",
    "            \n",
    "    return les_potential_human_box\n",
    "    \n",
    "    \n",
    "a = get_contours_human_ratio(BACK_SUB_FOLDER_PATH + \"/220.jpg\", disp=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class SVM:\n",
    "    \n",
    "    def __init__(self, les_im_path, replace=False, load_in_ram=True):\n",
    "        self.gt = read_gt(gt_path)\n",
    "        self.dataset_svm_path = \"dataset_svm\"\n",
    "        self.dataset_svm_neg_path = self.dataset_svm_path + \"_neg\"\n",
    "        \n",
    "        self.les_im_path = les_im_path\n",
    "        self.load_in_ram = load_in_ram\n",
    "        if self.load_in_ram:\n",
    "            self.persons = []\n",
    "            self.negative = []\n",
    "        \n",
    "        if replace:\n",
    "            # Remove dataseth\n",
    "            if os.path.isdir(self.dataset_svm_path):\n",
    "                shutil.rmtree(self.dataset_svm_path)\n",
    "            # remove negative dataset\n",
    "            if os.path.isdir(self.dataset_svm_neg_path):\n",
    "                shutil.rmtree(self.dataset_svm_neg_path)\n",
    "            \n",
    "            # Create empty dataset folders\n",
    "            os.mkdir(self.dataset_svm_path)\n",
    "            os.mkdir(self.dataset_svm_neg_path)\n",
    "            \n",
    "            self.dataset_prepare()    \n",
    "    \n",
    "    def write_random_areas(self, frame_path, frame, les_box):\n",
    "        \"\"\"\n",
    "        For SVMs, we have to generate negative samples. Thus, we need to generate images where there is not human\n",
    "        \"\"\"\n",
    "        frame_height, frame_width = frame.shape[:2]\n",
    "        \n",
    "        for id_box, box in enumerate(les_box):\n",
    "            \n",
    "            box_width = box.x_max - box.x_min\n",
    "            box_height = box.y_max - box.y_min\n",
    "            \n",
    "            target_is_ok = False\n",
    "            while not target_is_ok:\n",
    "                x_min_target = random.randint(0, frame_width - box_width-1)\n",
    "                y_min_target = random.randint(0, frame_height - box_height-1)\n",
    "                \n",
    "                box_target = Box(x_min_target, y_min_target, x_min_target + box_width, y_min_target + box_height)\n",
    "                \n",
    "                # Check if target box and other boxes are overlapping\n",
    "                for box2 in les_box:\n",
    "                    if not box2.is_overlap(box_target):\n",
    "                        # Stopping while\n",
    "                        target_is_ok = True\n",
    "                        # get neg image crop\n",
    "                    else:\n",
    "                        # If overlap, break the for loop to not reach write specs and set false ok target\n",
    "                        target_is_ok = False\n",
    "                        break\n",
    "                        \n",
    "                im_crop = frame[box_target.y_min:box_target.y_max, box_target.x_min:box_target.x_max]\n",
    "                im_crop = cv2.resize(im_crop,(WIDTH_SAMPLE_SVM,HEIGHT_SAMPLE_SVM))\n",
    "                \n",
    "                crop_path = self.dataset_svm_neg_path + '/' + frame_path.split(\"/\")[-1].replace(\".jpg\", \"_\" + str(id_box) + \".jpg\")\n",
    "                cv2.imwrite(crop_path, im_crop)\n",
    "\n",
    "                if self.load_in_ram:\n",
    "                    self.negative.append(im_crop)\n",
    "\n",
    "    \n",
    "    def dataset_prepare(self):\n",
    "        print(\"Extracting persons\")\n",
    "        for id_frame, frame_path in enumerate(self.les_im_path):\n",
    "            print(\"Frame #\" + str(id_frame), end=\"\\r\")\n",
    "            bbs_gt = annotations_for_frame(self.gt, id_frame)\n",
    "            black_gt = np.zeros((_H, _W))\n",
    "            frame = cv2.imread(frame_path)\n",
    "            \n",
    "            les_box = []\n",
    "            for id_bb, bb in enumerate(bbs_gt):\n",
    "                x, y = bb[2:4]\n",
    "                dx, dy = bb[4:6]\n",
    "                les_box.append(Box(x, y, x+dx, y+dy))\n",
    "                \n",
    "                im_crop = frame[y:y+dy, x:x+dx]\n",
    "                im_crop = cv2.resize(im_crop,(WIDTH_SAMPLE_SVM,HEIGHT_SAMPLE_SVM))\n",
    "                \n",
    "                crop_path = self.dataset_svm_path + '/' + frame_path.split(\"/\")[-1].replace(\".jpg\", \"_\" + str(id_bb) + \".jpg\")\n",
    "                cv2.imwrite(crop_path, im_crop)\n",
    "                if self.load_in_ram:\n",
    "                    self.persons.append(im_crop)\n",
    "                \n",
    "            self.write_random_areas(frame_path, frame, les_box)\n",
    "        print(\"Done\" + \" \"*20)\n",
    "                \n",
    "                \n",
    "    \n",
    "    def dataset_split(self):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data_images, digits.target)\n",
    "        print('Training data and target sizes: \\n{}, {}'.format(X_train.shape, y_train.shape))\n",
    "        print('Test data and target sizes: \\n{}, {}'.format(X_test.shape, y_test.shape))\n",
    "\n",
    "svm = SVM(tester.les_im_path, replace = True, load_in_ram=False)\n",
    "# svm.dataset_prepare(tester.les_im_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(svm.persons[0].shape)\n",
    "disp_image(svm.persons[4])\n",
    "disp_image(svm.negative[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply boxes to video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_box(les_box):\n",
    "    # If a box is above another, merge\n",
    "    to_process_box = les_box.copy()\n",
    "    les_new_box = []\n",
    "    while len(to_process_box) > 0:\n",
    "        box = to_process_box.pop(0)\n",
    "        # get x domain\n",
    "        les_box_mate = []\n",
    "        for box_mate in les_box:\n",
    "            # If overlap\n",
    "            if box.x_max > box_mate.x_min and box_mate.x_max > box.x_min:\n",
    "                les_box_mate.append(box_mate)\n",
    "                # Remove the soon merged box mate from the list of \"to merge\" boxes\n",
    "                try:\n",
    "                    to_process_box.remove(box_mate)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "        if len(les_box_mate) != 0:\n",
    "            # Get boundaries\n",
    "            x_min_mate = min([box_mate.x_min for box_mate in les_box_mate])\n",
    "            x_max_mate = max([box_mate.x_max for box_mate in les_box_mate])\n",
    "            y_min_mate = min([box_mate.y_min for box_mate in les_box_mate])\n",
    "            y_max_mate = max([box_mate.y_max for box_mate in les_box_mate])\n",
    "            les_new_box.append(Box(x_min_mate, y_min_mate, x_max_mate, y_max_mate))\n",
    "    \n",
    "    return les_new_box\n",
    "\n",
    "\n",
    "def intertia_consistency_box(les_whole_box, max_dist=5, n_consistency=5):\n",
    "    print(\"Checking box inertia and consistancy\")\n",
    "    # looping to check consistency between boxes n and n-1\n",
    "    # check if their is a box in the n+1 frame near to the box studied in the n frame\n",
    "    assert n_consistency > 0\n",
    "    \n",
    "    les_whole_box_intertia = []\n",
    "    for id_les_box in range(0, len(les_whole_box)-n_consistency):\n",
    "        print(\"#\" + str(id_les_box+1) + \"/\" + str(len(les_whole_box)), end=\"\\r\")\n",
    "        les_box_inertia = []\n",
    "        # get the study box\n",
    "        for study_box in les_whole_box[id_les_box]:\n",
    "            # init les_box list\n",
    "            # compute center of studied box\n",
    "            x = (study_box.x_max - study_box.x_min)/2\n",
    "            y = (study_box.y_max - study_box.y_min)/2\n",
    "            for target_box in les_whole_box[id_les_box+1]:\n",
    "                # compute center of targeted box\n",
    "                x_target = (target_box.x_max - target_box.x_min)/2\n",
    "                y_target = (target_box.y_max - target_box.y_min)/2\n",
    "                dist = np.linalg.norm( np.array([x, y]) - np.array([x_target, y_target]) )\n",
    "                if dist < max_dist:\n",
    "                    to_save = False\n",
    "                    # if own a near n+1 frame box, keep the studied box\n",
    "                    # if dual, check on second round\n",
    "                    if n_consistency > 1:\n",
    "                        for target_box_2 in les_whole_box[id_les_box+n_consistency]:\n",
    "                            x_target_2 = (target_box_2.x_max - target_box_2.x_min)/2\n",
    "                            y_target_2 = (target_box_2.y_max - target_box_2.y_min)/2\n",
    "                            dist = np.linalg.norm( np.array([x, y]) - np.array([x_target_2, y_target_2]) )\n",
    "                            if dist < max_dist*n_consistency:\n",
    "                                to_save=True\n",
    "                    else:\n",
    "                        to_save=True\n",
    "                        \n",
    "                    if to_save:\n",
    "                        les_box_inertia.append(study_box)\n",
    "                        break            \n",
    "        \n",
    "        # Store last frame boxes\n",
    "        les_whole_box_intertia.append(les_box_inertia)\n",
    "    print(\"Done\" + \" \"*20)\n",
    "    return les_whole_box_intertia\n",
    "\n",
    "\n",
    "def apply_box_to_video(les_im_path, les_im_backsub_path, video_name=\"apply_box\"):\n",
    "    \n",
    "    height,width,layers=cv2.imread(les_im_path[0]).shape\n",
    "\n",
    "    # Create video object\n",
    "    fourcc = cv2.VideoWriter_fourcc('M','J','P','G')\n",
    "    video = cv2.VideoWriter(video_name + '.avi', fourcc, fps=25, frameSize=(width,height))\n",
    "    \n",
    "    print(\"Computing boxes: ratio -> merging -> ratio\")\n",
    "    les_whole_box = []\n",
    "    # Computing background\n",
    "    for id_im, im_path in enumerate(les_im_path):\n",
    "        print(\"#\" + str(id_im+1) + \"/\" + str(len(les_im_path)), end=\"\\r\")\n",
    "        im = cv2.imread(im_path)\n",
    "        \n",
    "        # Get box\n",
    "        les_box = get_contours_human_ratio(les_im_backsub_path[id_im])\n",
    "        \n",
    "        # Merge box\n",
    "        merged_box = merge_box(les_box)\n",
    "        \n",
    "        # Get ride of out of ratio boxes\n",
    "        les_merged_ratio_box = []\n",
    "        for box in merged_box:\n",
    "            merged_ratio_box = filter_ratio(box, 1, 5)\n",
    "            if merged_ratio_box is not None:\n",
    "                les_merged_ratio_box.append(merged_ratio_box)\n",
    "        \n",
    "        les_whole_box.append(les_merged_ratio_box)\n",
    "    print(\"Done\" + \" \"*20)\n",
    "    \n",
    "    les_whole_box_inertia = intertia_consistency_box(les_whole_box.copy(), max_dist=MAX_DIST_CONSIST, n_consistency=N_CONSISTENCY)\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    print(\"Writing video\")\n",
    "    for id_les_box, les_box in enumerate(les_whole_box_inertia):\n",
    "        print(\"#\" + str(id_les_box+1) + \"/\" + str(len(les_whole_box_inertia)), end=\"\\r\")\n",
    "        # write box into image\n",
    "        im = cv2.imread(les_im_path[id_les_box])\n",
    "        # Write frame id in im\n",
    "        cv2.putText(im, str(id_les_box),(10,height), font, 3,(255,255,255),2,cv2.LINE_AA)\n",
    "        for box in les_box:\n",
    "            cv2.rectangle(\n",
    "                im, \n",
    "                (box.x_min, box.y_min), \n",
    "                (box.x_max, box.y_max),\n",
    "                (255, 0, 0),\n",
    "                3\n",
    "            )\n",
    "\n",
    "        # write image to video\n",
    "        video.write(im)\n",
    "    video.release()\n",
    "    print(\"Done\" + \" \"*20)\n",
    "    \n",
    "    return les_whole_box_inertia\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interface:\n",
    "    def __init__(self, data_root=data_root, back_sub_folder_path=BACK_SUB_FOLDER_PATH):\n",
    "        self.dataset_path = data_root\n",
    "        self.dataset_backsub_path = back_sub_folder_path\n",
    "        self.les_im_path = self.get_dataset_im_path(self.dataset_path)\n",
    "        self.les_im_backsub_path = self.get_dataset_im_path(self.dataset_backsub_path)\n",
    "    \n",
    "    def get_dataset_im_path(self, path):\n",
    "        \"\"\"\n",
    "        Get images path using glob\n",
    "        :param path: string, dataset folder\n",
    "        :return les_im_path: list of string, lsit sorted of every .jpg file in the dataset\n",
    "        \"\"\"\n",
    "        les_im_path = glob.glob(path + \"/*.jpg\")\n",
    "        les_im_path.sort()\n",
    "        return les_im_path\n",
    "    \n",
    "    def test_hog(self):\n",
    "        test_im = self.les_im_path[0]\n",
    "        hog(test_im)\n",
    "    \n",
    "    def background_substraction(self):\n",
    "        background_substraction(\n",
    "            self.les_im_path, \n",
    "            video_name=\"backgroud_substraction\", \n",
    "            back_sub_folder=self.dataset_backsub_path\n",
    "        )\n",
    "        self.les_im_backsub_path = self.get_dataset_im_path(self.dataset_backsub_path)\n",
    "        \n",
    "    def test_apply_box_to_video(self):\n",
    "        apply_box_to_video(self.les_im_path, self.les_im_backsub_path)\n",
    "\n",
    "tester = Interface()\n",
    "# tester.background_substraction()\n",
    "# tester.test_apply_box_to_video()\n",
    "# tester.test_disp_im()\n",
    "# tester.test_hog()\n",
    "# tester.background_substraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pedestrians(data_root, _W, _H, _N):\n",
    "    ''' Return a list of bounding boxes in the format frame, bb_id, x,y,dx,dy '''\n",
    "    \n",
    "    if not os.path.isdir(BACK_SUB_FOLDER_PATH):\n",
    "        # Generate back sub folder\n",
    "        pass\n",
    "    \n",
    "    interface = Interface(data_root, BACK_SUB_FOLDER_PATH)\n",
    "    # print(interface.les_im_path)\n",
    "    \n",
    "    les_wholes_box_id = apply_box_to_video(\n",
    "        les_im_path=interface.les_im_path, \n",
    "        les_im_backsub_path=interface.les_im_backsub_path,\n",
    "        video_name=\"apply_box\")\n",
    "    \n",
    "    # Putting in format asked by evaluation function\n",
    "    les_pedestrians = []\n",
    "    for id_frame, frame_boxes in enumerate(les_wholes_box_id):\n",
    "        for id_box, box in enumerate(frame_boxes):\n",
    "            pedestrian = [id_frame+1, id_box+1, box.x_min, box.y_min, box.x_max-box.x_min, box.y_max-box.y_min]\n",
    "            les_pedestrians.append(pedestrian)\n",
    "    \n",
    "    return les_pedestrians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N.Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORING = True\n",
    "if SCORING:\n",
    "    gt = read_gt(gt_path)\n",
    "    check_id = 309\n",
    "    show_annotation(gt, check_id)\n",
    "\n",
    "    print('A perfect score... {}'.format(evaluate_solution(gt, gt, _N)))\n",
    "\n",
    "    # your solution will be tested simply by changing the dataset\n",
    "    # and changing the module, i.e., the following has to work \n",
    "    # with simply using your module \n",
    "    sol = pedestrians(data_root, _W, _H, _N)\n",
    "    print('A great score! {}'.format(evaluate_solution(sol, gt, _N)))\n",
    "    show_annotation(sol, check_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "def read_seq(path):\n",
    "    \n",
    "    def read_header(ifile):\n",
    "        feed = ifile.read(4)\n",
    "        norpix = ifile.read(24)\n",
    "        version = struct.unpack('@i', ifile.read(4))\n",
    "        length = struct.unpack('@i', ifile.read(4))\n",
    "        assert(length != 1024)\n",
    "        descr = ifile.read(512)\n",
    "        params = [struct.unpack('@i', ifile.read(4))[0] for i in range(0,9)]\n",
    "        fps = struct.unpack('@d', ifile.read(8))\n",
    "        # skipping the rest\n",
    "        ifile.read(432)\n",
    "        image_ext = {100:'raw', 102:'jpg',201:'jpg',1:'png',2:'png'}\n",
    "        return {'w':params[0],'h':params[1],\n",
    "                'bdepth':params[2],\n",
    "                'ext':image_ext[params[5]],\n",
    "                'format':params[5],\n",
    "                'size':params[4],\n",
    "                'true_size':params[8],\n",
    "                'num_frames':params[6]}\n",
    "    \n",
    "    ifile = open(path, 'rb')\n",
    "    params = read_header(ifile)\n",
    "    bytes = open(path, 'rb').read()\n",
    "\n",
    "    # this is freaking magic, but it works\n",
    "    extra = 8\n",
    "    s = 1024\n",
    "    seek = [0]*(params['num_frames']+1)\n",
    "    seek[0] = 1024\n",
    "    \n",
    "    images = []\n",
    "    \n",
    "    for i in range(0, params['num_frames']-1):\n",
    "        tmp = struct.unpack_from('@I', bytes[s:s+4])[0]\n",
    "        s = seek[i] + tmp + extra\n",
    "        if i == 0:\n",
    "            val = struct.unpack_from('@B', bytes[s:s+1])[0]\n",
    "            if val != 0:\n",
    "                s -= 4\n",
    "            else:\n",
    "                extra += 8\n",
    "                s += 8\n",
    "        seek[i+1] = s\n",
    "        nbytes = struct.unpack_from('@i', bytes[s:s+4])[0]\n",
    "        I = bytes[s+4:s+nbytes]\n",
    "        \n",
    "        tmp_file = '/tmp/img%d.jpg' % i\n",
    "        open(tmp_file, 'wb+').write(I)\n",
    "        \n",
    "        img = cv2.imread(tmp_file)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        images.append(img)\n",
    "    return images\n",
    "\n",
    "# a = read_seq(\"dataset/set00/V000.seq\")[200]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
