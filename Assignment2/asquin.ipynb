{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pedestrians recognition without using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import glob\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "from skimage import exposure\n",
    "from skimage import feature\n",
    "\n",
    "\n",
    "data_root = \"img1\"\n",
    "BACK_SUB_FOLDER_PATH = \"back_sub\"\n",
    "DATASET_SVM_PATH = \"dataset_svm\"\n",
    "VIDEO_NAME = \"asquin_apply_box\"\n",
    "MIN_HUMAN_RATIO = 1\n",
    "MAX_HUMAN_RATIO = 12\n",
    "MAX_RATIO_MERGE = MAX_HUMAN_RATIO\n",
    "\n",
    "MAX_DIST_CONSIST = 10\n",
    "N_CONSISTENCY = 1\n",
    "\n",
    "\n",
    "gt_path = './gt/gt.txt'\n",
    "\n",
    "_W = 1280\n",
    "_H = 960\n",
    "_N = 684 # number of frames\n",
    "\n",
    "WIDTH_SAMPLE_SVM = 15\n",
    "HEIGHT_SAMPLE_SVM = 30\n",
    "\n",
    "print(\"Starting asquin.py / .ipynb\")\n",
    "print(\"Please be sure to be in the same folder than 'gt' and 'img1.\")\n",
    "print(\"The process leads to a SVM machine training along with pictures filters and extraction.\")\n",
    "print(\"Please be aware that folders are going to be created, such as\", BACK_SUB_FOLDER_PATH, \"or\", DATASET_SVM_PATH)\n",
    "print(\"Afterward, please have a look at the output video:\", VIDEO_NAME + \".avi\")\n",
    "print(\"The total process may take up to 1 minutes. Thanks for waiting :)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and dataset function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_id(frame):\n",
    "    assert _N >= frame\n",
    "    return '{:03d}'.format(frame)\n",
    "\n",
    "\n",
    "def read_frame(root, frame):\n",
    "    \"\"\"Read frames and create integer frame_id-s\"\"\"\n",
    "    assert _N >= frame\n",
    "    return cv2.imread(os.path.join(root,format_id(frame)+'.jpg'), cv2.IMREAD_UNCHANGED)\n",
    "    \n",
    "\n",
    "def read_gt(filename):\n",
    "    \"\"\"Read gt and create list of bb-s\"\"\"\n",
    "    assert os.path.exists(filename)\n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    # truncate data (last columns are not needed)\n",
    "    return [list(map(lambda x: int(x), line.split(',')[:6])) for line in lines]\n",
    "\n",
    "\n",
    "def annotations_for_frame(solution, frame):\n",
    "    assert _N >= frame\n",
    "    return [bb for bb in solution if int(bb[0])==int(frame)]\n",
    "\n",
    "\n",
    "def evaluate_solution(gt, solution, N):\n",
    "    \"\"\"Caclulate evaluation metric\"\"\"\n",
    "    score = []\n",
    "    #for frame in [300]:\n",
    "    for frame in range(1, N):\n",
    "        bbs_sol = annotations_for_frame(solution, frame)\n",
    "        bbs_gt = annotations_for_frame(gt, frame)\n",
    "        black_sol = np.zeros((_H, _W))\n",
    "        black_gt = np.zeros((_H, _W))\n",
    "        for bb in bbs_sol:\n",
    "            x, y = bb[2:4]\n",
    "            dx, dy = bb[4:6]\n",
    "            cv2.rectangle(black_sol, (x, y), (x+dx, y+dy), (255), -1)\n",
    "        for bb in bbs_gt:\n",
    "            x, y = bb[2:4]\n",
    "            dx, dy = bb[4:6]\n",
    "            cv2.rectangle(black_gt, (x, y), (x+dx, y+dy), (255), -1)\n",
    "        # intersection over union\n",
    "        intersection = black_sol * black_gt\n",
    "        intersection[intersection > 0.5] = 1\n",
    "        union = black_sol + black_gt\n",
    "        union[union > 0.5] = 1\n",
    "        if not union.any():\n",
    "            continue\n",
    "        score.append(intersection.sum()/union.sum())\n",
    "        \n",
    "        \n",
    "    return np.asarray(score).mean()\n",
    "    \n",
    "\n",
    "def show_annotation(solution, frame):\n",
    "    assert _N >= frame\n",
    "    im = read_frame(data_root, frame)\n",
    "    bbs = annotations_for_frame(solution, frame)\n",
    "    for bb in bbs:\n",
    "        x, y = bb[2:4]\n",
    "        dx, dy = bb[4:6]\n",
    "        cv2.rectangle(im, (x, y), (x+dx, y+dy), (0,255,0), 10)\n",
    "    plt.imshow(im)\n",
    "    plt.title('Annotations for frame {}.'.format(frame))\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disp_image(im):\n",
    "    \"\"\"\n",
    "    Just display an image using matplotlib pyplot\n",
    "    :param im: image, np.array\n",
    "    \"\"\"\n",
    "    imgplot = plt.imshow(im)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOG function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hog(im, disp=False):\n",
    "    \"\"\"\n",
    "    Compute the HOG transformation of the image given in input\n",
    "    :param im: image, np.array\n",
    "    :param disp: bool, default=False, set to True to visualise the HOG transform\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    if disp:\n",
    "        # extract Histogram of Oriented Gradients from the logo\n",
    "        (H, hogImage) = feature.hog(gray, \n",
    "                                    orientations=9, \n",
    "                                    pixels_per_cell=(2, 2),\n",
    "                                    cells_per_block=(1, 1), \n",
    "                                    transform_sqrt=True, \n",
    "                                    block_norm=\"L1\", \n",
    "                                    visualise=True)\n",
    "\n",
    "        hogImage = exposure.rescale_intensity(hogImage, out_range=(0, 255))\n",
    "        hogImage = hogImage.astype(\"uint8\")\n",
    "\n",
    "        disp_image(logo)\n",
    "        disp_image(hogImage)\n",
    "    else:\n",
    "        H = feature.hog(gray, \n",
    "                        orientations=9, \n",
    "                        pixels_per_cell=(2, 2),\n",
    "                        cells_per_block=(1, 1), \n",
    "                        transform_sqrt=True, \n",
    "                        block_norm=\"L1\")\n",
    "    \n",
    "    return H\n",
    "    \n",
    "# im = cv2.imread(\"dataset_svm/002_0.jpg\")\n",
    "# hog(im, disp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background substraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def background_substraction(les_im_path, video_name=None, back_sub_folder=\"back_sub\"):\n",
    "    \"\"\"\n",
    "    Generate a video with background substraction\n",
    "    :param les_im_path: list of string, images path\n",
    "    :param video_name: string, default = None, if set, will save a video of the backgroud substraction to 'video_name'.avi\n",
    "    :param back_sub_folder: string, default = 'back_sub', folder used to save background substracted images\n",
    "    \n",
    "    OUTPUT:\n",
    "        back_sub/XXX.jpg : images of the background substraction\n",
    "        \n",
    "        if video_name != None:\n",
    "            video_name.avi : video of the background substraction\n",
    "    \"\"\"\n",
    "    print(\"Running background substraction:\")\n",
    "    # Create background frame\n",
    "    fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "    \n",
    "    # Create output folder if doesn't already exist\n",
    "    try:\n",
    "        os.mkdir(back_sub_folder)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    if video_name is not None:\n",
    "        # Get video size\n",
    "        height,width,layers=cv2.imread(les_im_path[0]).shape\n",
    "\n",
    "        # Create video object\n",
    "        fourcc = cv2.VideoWriter_fourcc('M','J','P','G')\n",
    "        video = cv2.VideoWriter(video_name + '.avi', fourcc, fps=25, frameSize=(width,height), isColor=False)\n",
    "    \n",
    "    # Computing background\n",
    "    for id_im, im_path in enumerate(les_im_path):\n",
    "        print(\"#\" + str(id_im), end=\"\\r\")\n",
    "        im = cv2.imread(im_path)\n",
    "        fgmask = fgbg.apply(im)\n",
    "        \n",
    "        cv2.imwrite(back_sub_folder + '/' + im_path.split(\"/\")[-1], fgmask)\n",
    "        \n",
    "        if video_name is not None:\n",
    "            # Writing frame to video\n",
    "            video.write(fgmask)\n",
    "    \n",
    "    if video_name is not None:\n",
    "        # Close video\n",
    "        video.release()\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interface:\n",
    "    \"\"\"\n",
    "    Load in attributes useful data, such as images paths. \n",
    "    Also implement interfacing with background substraction or the apply box to video function\n",
    "    \n",
    "    :param data_root: string, where is the img dataset\n",
    "    :param back_sub_folder: string, where should be stored the background substracted images\n",
    "    \"\"\"\n",
    "    def __init__(self, data_root=data_root, back_sub_folder_path=BACK_SUB_FOLDER_PATH):\n",
    "        self.dataset_path = data_root\n",
    "        self.dataset_backsub_path = back_sub_folder_path\n",
    "        self.les_im_path = self.get_dataset_im_path(self.dataset_path)\n",
    "        self.les_im_backsub_path = self.get_dataset_im_path(self.dataset_backsub_path)\n",
    "    \n",
    "    def get_dataset_im_path(self, path):\n",
    "        \"\"\"\n",
    "        Get images path using glob\n",
    "        :param path: string, dataset folder\n",
    "        :return les_im_path: list of string, lsit sorted of every .jpg file in the dataset\n",
    "        \"\"\"\n",
    "        les_im_path = glob.glob(path + \"/*.jpg\")\n",
    "        les_im_path.sort()\n",
    "        return les_im_path\n",
    "    \n",
    "    def background_substraction(self):\n",
    "        background_substraction(\n",
    "            self.les_im_path, \n",
    "            video_name=\"backgroud_substraction\", \n",
    "            back_sub_folder=self.dataset_backsub_path\n",
    "        )\n",
    "        self.les_im_backsub_path = self.get_dataset_im_path(self.dataset_backsub_path)\n",
    "        \n",
    "    def apply_box_to_video(self):\n",
    "        apply_box_to_video(self.les_im_path, self.les_im_backsub_path)\n",
    "\n",
    "interfacer = Interface()\n",
    "interfacer.background_substraction()\n",
    "# interfacer.apply_box_to_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get human shaped contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Box:\n",
    "    \"\"\"\n",
    "    Object to store a box by its boundaries (Point of min x and y, Point of max x and y)\n",
    "    \n",
    "    :param x_min: int, x minimal value of the box\n",
    "    :param y_min: int, y minimal value of the box\n",
    "    :param x_max: int, x maximal value of the box\n",
    "    :param y_max: int, x maximal value of the box\n",
    "    \"\"\"\n",
    "    def __init__(self, x_min, y_min, x_max, y_max):\n",
    "        self.x_min = x_min\n",
    "        self.y_min = y_min\n",
    "        self.x_max = x_max\n",
    "        self.y_max = y_max\n",
    "    \n",
    "    def __repr__(self):\n",
    "        ret = \"x_min: \" + str(self.x_min) + \", y_min: \" + str(self.y_min) + \", x_max: \" + str(self.x_max) + \", y_max: \" + str(self.y_max)\n",
    "        return ret\n",
    "    \n",
    "    def is_overlap(self, other):\n",
    "        \"\"\"\n",
    "        Check if 2 Box object are overlapping\n",
    "        :param other: Box, the other box to evaluate a overlap with\n",
    "        :return is_overlapping: True if the boxes are overlapping, False else.\n",
    "        \"\"\"\n",
    "        return not (self.x_max < other.x_min or self.x_min > other.x_max or self.y_max < other.y_min or self.y_min > other.y_max)\n",
    "\n",
    "        \n",
    "def filter_ratio(box, ratio_min, ratio_max, disp=False):\n",
    "    \"\"\"\n",
    "    Return the box if in ratio_min and ratio_max, return None else.\n",
    "    :param box: the box to check\n",
    "    :param ratio_min: minimum accepted ratio\n",
    "    :param ratio_max: maximum accepted ratio\n",
    "    \"\"\"\n",
    "    width = box.x_max - box.x_min\n",
    "    height = box.y_max - box.y_min\n",
    "\n",
    "    # ratio\n",
    "    ratio = height / width\n",
    "\n",
    "    if ratio > ratio_min and ratio < ratio_max:\n",
    "        return box\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "        \n",
    "def get_contours_human_ratio(im_path, ratio=False, disp=False):\n",
    "    \"\"\"\n",
    "    Load the image, get the contour of the shape in it and check if they are human-shape like\n",
    "    \n",
    "    :param im_path: string, path to the background_substracted image to analyse.\n",
    "    :param disp: bool, default=False, set to True to visualize the contours.\n",
    "    :return les_potential_human_box: boxes that could be human like in shape (number_frame, box in this frame)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Load image and resize witout changing ratio\n",
    "    im = cv2.imread(im_path)\n",
    "    height, width = im.shape[:2]\n",
    "    new_width = 500\n",
    "    new_height = new_width*height//width\n",
    "    im = cv2.resize(im,(new_width, new_height), interpolation = cv2.INTER_CUBIC)\n",
    "    \n",
    "    # Change to gray and apply both gaussian and threshold filter\n",
    "    im_gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "    blurred_im = cv2.GaussianBlur(im_gray, (1, 1), 0)\n",
    "    ret,thresh = cv2.threshold(blurred_im, 220, 255, 0)\n",
    "    \n",
    "    # Compute contours\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Get dimension of main contours\n",
    "    les_potential_human_box = []\n",
    "    for cnt in contours:\n",
    "        # Compute area size\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if area > 3:\n",
    "            # remove overdimension of contours\n",
    "            cnt_low = cnt[:, 0]\n",
    "            \n",
    "            if disp:\n",
    "                print(area)\n",
    "                im2 = cv2.drawContours(im, cnt, -1, (255,0,0), 2)\n",
    "                disp_image(im2)\n",
    "    \n",
    "            # contour width\n",
    "            x_max = np.max(cnt_low[:, 0])*width//new_width\n",
    "            x_min = np.min(cnt_low[:, 0])*width//new_width\n",
    "            # contour height\n",
    "            y_max = np.max(cnt_low[:, 1])*height//new_height\n",
    "            y_min = np.min(cnt_low[:, 1])*height//new_height\n",
    "            box = Box(x_min, y_min, x_max, y_max)\n",
    "            \n",
    "            if ratio:\n",
    "                potential_human_box = filter_ratio(box, MIN_HUMAN_RATIO, MAX_HUMAN_RATIO)\n",
    "                if potential_human_box is not None:\n",
    "                    les_potential_human_box.append(potential_human_box)\n",
    "            else:\n",
    "                les_potential_human_box.append(box)\n",
    "            \n",
    "    return les_potential_human_box\n",
    "    \n",
    "    \n",
    "# a = get_contours_human_ratio(BACK_SUB_FOLDER_PATH + \"/287.jpg\", disp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "class SVM:\n",
    "    \"\"\"\n",
    "    Implement a SVM dataset preparation and a SVM training\n",
    "    :param les_im_path: list of string, path to original images\n",
    "    :param replace: bool, default=False, set to True to visualize the SVM dataset images\n",
    "    :param load_in_ram: bool, default=True, store the dataset in the RAM\n",
    "    \"\"\"\n",
    "    def __init__(self, les_im_path, replace=False, load_in_ram=True):\n",
    "        self.gt = read_gt(gt_path)\n",
    "        self.dataset_svm_path = DATASET_SVM_PATH\n",
    "        self.dataset_svm_neg_path = self.dataset_svm_path + \"_neg\"\n",
    "        self.replace = replace\n",
    "        \n",
    "        self.les_im_path = les_im_path\n",
    "        self.load_in_ram = load_in_ram\n",
    "        if self.load_in_ram:\n",
    "            self.data = []\n",
    "            self.labels = []\n",
    "        \n",
    "        if self.replace:\n",
    "            # Remove dataseth\n",
    "            if os.path.isdir(self.dataset_svm_path):\n",
    "                shutil.rmtree(self.dataset_svm_path)\n",
    "            # remove negative dataset\n",
    "            if os.path.isdir(self.dataset_svm_neg_path):\n",
    "                shutil.rmtree(self.dataset_svm_neg_path)\n",
    "            \n",
    "            # Create empty dataset folders\n",
    "            os.mkdir(self.dataset_svm_path)\n",
    "            os.mkdir(self.dataset_svm_neg_path)\n",
    "    \n",
    "    def write_random_areas(self, frame_path, frame, les_box):\n",
    "        \"\"\"\n",
    "        For SVMs, we have to generate negative samples. \n",
    "        Thus, we need to generate images where there is not human.\n",
    "        \n",
    "        :param frame_path: string, path to the image\n",
    "        :param les_box: box containing humans (boxes to avoid in the random areas generation)\n",
    "        \"\"\"\n",
    "        frame_height, frame_width = frame.shape[:2]\n",
    "        \n",
    "        for id_box, box in enumerate(les_box):\n",
    "            \n",
    "            box_width = box.x_max - box.x_min\n",
    "            box_height = box.y_max - box.y_min\n",
    "            \n",
    "            target_is_ok = False\n",
    "            while not target_is_ok:\n",
    "                x_min_target = random.randint(0, frame_width - box_width-1)\n",
    "                y_min_target = random.randint(0, frame_height - box_height-1)\n",
    "                \n",
    "                box_target = Box(x_min_target, y_min_target, x_min_target + box_width, y_min_target + box_height)\n",
    "                \n",
    "                # Check if target box and other boxes are overlapping\n",
    "                for box2 in les_box:\n",
    "                    if not box2.is_overlap(box_target):\n",
    "                        # Stopping while\n",
    "                        target_is_ok = True\n",
    "                        # get neg image crop\n",
    "                    else:\n",
    "                        # If overlap, break the for loop to not reach write specs and set false ok target\n",
    "                        target_is_ok = False\n",
    "                        break\n",
    "                        \n",
    "                im_crop = frame[box_target.y_min:box_target.y_max, box_target.x_min:box_target.x_max]\n",
    "                im_crop = cv2.resize(im_crop,(WIDTH_SAMPLE_SVM,HEIGHT_SAMPLE_SVM))\n",
    "                \n",
    "                crop_path = self.dataset_svm_neg_path + '/' + frame_path.split(\"/\")[-1].replace(\".jpg\", \"_\" + str(id_box) + \".jpg\")\n",
    "                if self.replace:\n",
    "                    cv2.imwrite(crop_path, im_crop)\n",
    "\n",
    "                if self.load_in_ram:\n",
    "                    # self.negative.append(im_crop)\n",
    "                    self.data.append(im_crop)\n",
    "                    self.labels.append(0)\n",
    "\n",
    "    \n",
    "    def dataset_prepare(self):\n",
    "        \"\"\"\n",
    "        SVM dataset preparation.\n",
    "        Will extract humans in the dataset_svm_path and negative samples in the dataset_svm_path_neg folders\n",
    "        with an image shape of (WIDTH_SAMPLE_SVM, HEIGHT_SAMPLE_SVM)\n",
    "        \"\"\"\n",
    "        print(\"Extracting SVM dataset : positive (persons) and negative\")\n",
    "        for id_frame, frame_path in enumerate(self.les_im_path):\n",
    "            print(\"Frame #\" + str(id_frame), end=\"\\r\")\n",
    "            bbs_gt = annotations_for_frame(self.gt, id_frame)\n",
    "            black_gt = np.zeros((_H, _W))\n",
    "            frame = cv2.imread(frame_path)\n",
    "            \n",
    "            les_box = []\n",
    "            for id_bb, bb in enumerate(bbs_gt):\n",
    "                x, y = bb[2:4]\n",
    "                dx, dy = bb[4:6]\n",
    "                les_box.append(Box(x, y, x+dx, y+dy))\n",
    "                \n",
    "                im_crop = frame[y:y+dy, x:x+dx]\n",
    "                im_crop = cv2.resize(im_crop,(WIDTH_SAMPLE_SVM,HEIGHT_SAMPLE_SVM))\n",
    "                \n",
    "                crop_path = self.dataset_svm_path + '/' + frame_path.split(\"/\")[-1].replace(\".jpg\", \"_\" + str(id_bb) + \".jpg\")\n",
    "                cv2.imwrite(crop_path, im_crop)\n",
    "                if self.load_in_ram:\n",
    "                    self.data.append(im_crop)\n",
    "                    self.labels.append(1)\n",
    "                \n",
    "            self.write_random_areas(frame_path, frame, les_box)\n",
    "        print(\"Done\" + \" \"*20)\n",
    "                \n",
    "    def train(self, test_size=0.2):\n",
    "        \"\"\"\n",
    "        Train the SVM after having applied a HOG transform to the images of the dataset.\n",
    "        :param test_size: test size ratio of the test dataset to check the quality of the SVM classifier.\n",
    "        \"\"\"\n",
    "        # Apply hog transform to images\n",
    "        data_prep = np.array(list(map(hog, self.data)))\n",
    "        labels_prep = np.array(self.labels)\n",
    "        \n",
    "        # Suffle the dataset\n",
    "        data_prep, labels_prep = shuffle(data_prep, labels_prep, random_state=0)\n",
    "        \n",
    "        # Split in test and train\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            data_prep, labels_prep, test_size=test_size)\n",
    "        \n",
    "        print('Training data and target sizes: \\n{}, {}'.format(self.X_train.shape, self.y_train.shape))\n",
    "        print('Test data and target sizes: \\n{}, {}'.format(self.X_test.shape, self.y_test.shape))\n",
    "        \n",
    "        self.classifier = svm.SVC(gamma=\"auto\", verbose=True)\n",
    "        #fit to the trainin data\n",
    "        self.classifier.fit(self.X_train,self.y_train)\n",
    "    \n",
    "    def check_accuracy(self):\n",
    "        y_pred = svm_trainer.classifier.predict(svm_trainer.X_test)\n",
    "        print(\"Accuracy\\n\", metrics.classification_report(svm_trainer.y_test, y_pred))\n",
    "        print(\"SVM accuracy:\",metrics.accuracy_score(svm_trainer.y_test, y_pred))\n",
    "        \n",
    "        \n",
    "\n",
    "svm_trainer = SVM(interfacer.les_im_path, replace = False, load_in_ram=True)\n",
    "svm_trainer.dataset_prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_trainer.train()\n",
    "svm_trainer.check_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply boxes to video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_box(les_box):\n",
    "    \"\"\"\n",
    "    Merge boxes that are near each others with one above the other.\n",
    "    :param les_box: list of Box, boxes to check and merge\n",
    "    :return les_new_box: list of merged boxes\n",
    "    \"\"\"\n",
    "    # If a box is above another, merge\n",
    "    to_process_box = les_box.copy()\n",
    "    les_new_box = []\n",
    "    while len(to_process_box) > 0:\n",
    "        box = to_process_box.pop(0)\n",
    "        # get x domain\n",
    "        les_box_mate = []\n",
    "        for box_mate in les_box:\n",
    "            if box.x_max > box_mate.x_min and box_mate.x_max > box.x_min:\n",
    "                les_box_mate.append(box_mate)\n",
    "                # Remove the soon merged box mate from the list of \"to merge\" boxes\n",
    "                try:\n",
    "                    to_process_box.remove(box_mate)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "        if len(les_box_mate) != 0:\n",
    "            # Get boundaries\n",
    "            x_min_mate = min([box_mate.x_min for box_mate in les_box_mate])\n",
    "            x_max_mate = max([box_mate.x_max for box_mate in les_box_mate])\n",
    "            y_min_mate = min([box_mate.y_min for box_mate in les_box_mate])\n",
    "            y_max_mate = max([box_mate.y_max for box_mate in les_box_mate])\n",
    "            les_new_box.append(Box(x_min_mate, y_min_mate, x_max_mate, y_max_mate))\n",
    "    \n",
    "    return les_new_box\n",
    "\n",
    "\n",
    "def inertia_consistency_box(les_whole_box, max_dist=5, n_consistency=1):\n",
    "    \"\"\"\n",
    "    Check if there is an inertia in the box evolution: that means if they are in the neighborhood after\n",
    "    a few frames.\n",
    "    \n",
    "    :param les_whole_box: list of Boxes to check\n",
    "    :param max_dist: int, maximim distance between two boxes, between two frames, for the box to be kept\n",
    "    :param n_consistency: int, default=1, set to n > 1 if you want a second check after n frames at distance n*max_dist.\n",
    "    \"\"\"\n",
    "    print(\"Checking box inertia and consistancy\")\n",
    "    # looping to check consistency between boxes n and n-1\n",
    "    # check if their is a box in the n+1 frame near to the box studied in the n frame\n",
    "    assert n_consistency > 0\n",
    "    \n",
    "    les_whole_box_inertia = []\n",
    "    for id_les_box in range(0, len(les_whole_box)-n_consistency):\n",
    "        print(\"#\" + str(id_les_box+1) + \"/\" + str(len(les_whole_box)), end=\"\\r\")\n",
    "        les_box_inertia = []\n",
    "        # get the study box\n",
    "        for study_box in les_whole_box[id_les_box]:\n",
    "            # init les_box list\n",
    "            # compute center of studied box\n",
    "            x = (study_box.x_max - study_box.x_min)/2\n",
    "            y = (study_box.y_max - study_box.y_min)/2\n",
    "            for target_box in les_whole_box[id_les_box+1]:\n",
    "                # compute center of targeted box\n",
    "                x_target = (target_box.x_max - target_box.x_min)/2\n",
    "                y_target = (target_box.y_max - target_box.y_min)/2\n",
    "                dist = np.linalg.norm( np.array([x, y]) - np.array([x_target, y_target]) )\n",
    "                if dist < max_dist:\n",
    "                    to_save = False\n",
    "                    # if own a near n+1 frame box, keep the studied box\n",
    "                    # if dual, check on second round\n",
    "                    if n_consistency > 1:\n",
    "                        for target_box_2 in les_whole_box[id_les_box+n_consistency]:\n",
    "                            x_target_2 = (target_box_2.x_max - target_box_2.x_min)/2\n",
    "                            y_target_2 = (target_box_2.y_max - target_box_2.y_min)/2\n",
    "                            dist = np.linalg.norm( np.array([x, y]) - np.array([x_target_2, y_target_2]) )\n",
    "                            if dist < max_dist*n_consistency:\n",
    "                                to_save=True\n",
    "                    else:\n",
    "                        to_save=True\n",
    "                        \n",
    "                    if to_save:\n",
    "                        les_box_inertia.append(study_box)\n",
    "                        break            \n",
    "        \n",
    "        # Store last frame boxes\n",
    "        les_whole_box_inertia.append(les_box_inertia)\n",
    "    print(\"Done\" + \" \"*20)\n",
    "    return les_whole_box_inertia\n",
    "\n",
    "def check_is_human_svm(im, box):\n",
    "    \"\"\"\n",
    "    Use the SVM training to guess if the object in the image in the box is a human.\n",
    "    :param im: image, np.array: original image to study\n",
    "    :param box: Box, box that is outbounding the element to study\n",
    "    :return pred_bool: bool, True if guess that object is human, False if suspect it's not human\n",
    "    \"\"\"\n",
    "    \n",
    "    im_crop = im[box.y_min:box.y_max, box.x_min:box.x_max]\n",
    "    im_crop = cv2.resize(im_crop,(WIDTH_SAMPLE_SVM,HEIGHT_SAMPLE_SVM))\n",
    "    x_prep = np.array(hog(im_crop)).reshape(1, -1)\n",
    "    \n",
    "    y_pred = svm_trainer.classifier.predict(x_prep)\n",
    "    \n",
    "    return y_pred[0] == 1\n",
    "\n",
    "\n",
    "def apply_box_to_video(les_im_path, les_im_backsub_path, video_name=\"apply_box\"):\n",
    "    \"\"\"\n",
    "    Apply the filters and box functions, along with the SVM function, to generate the boxes and apply \n",
    "    them to a video for a better visualization\n",
    "    \n",
    "    :les_im_path: list of string, path to studied images\n",
    "    :les_im_backsub_path: list of string, path to the background substracted images\n",
    "    :video_name: string, name of the video to write in order to visualize the algorithms efficiency\n",
    "    \"\"\"\n",
    "    \n",
    "    height,width,layers=cv2.imread(les_im_path[0]).shape\n",
    "\n",
    "    # Create video object\n",
    "    fourcc = cv2.VideoWriter_fourcc('M','J','P','G')\n",
    "    video = cv2.VideoWriter(video_name + '.avi', fourcc, fps=25, frameSize=(width,height))\n",
    "    \n",
    "    print(\"Computing boxes: contours For this project-> merging -> ratio -> SVM\")\n",
    "    les_whole_box = []\n",
    "    # Computing background\n",
    "    for id_im, im_path in enumerate(les_im_path):\n",
    "        print(\"#\" + str(id_im+1) + \"/\" + str(len(les_im_path)), end=\"\\r\")\n",
    "        im = cv2.imread(im_path)\n",
    "        \n",
    "        # Get box\n",
    "        les_box = get_contours_human_ratio(les_im_backsub_path[id_im])\n",
    "        \n",
    "        # Merge box\n",
    "        merged_box = merge_box(les_box)\n",
    "        \n",
    "        # Get ride of out of ratio boxes\n",
    "        les_merged_ratio_box = []\n",
    "        for box in merged_box:\n",
    "            merged_ratio_box = filter_ratio(box, MIN_HUMAN_RATIO, MAX_HUMAN_RATIO)\n",
    "            if merged_ratio_box is not None:\n",
    "                if check_is_human_svm(im, box):\n",
    "                    les_merged_ratio_box.append(merged_ratio_box)\n",
    "        \n",
    "        les_whole_box.append(les_merged_ratio_box)\n",
    "    print(\"Done\" + \" \"*20)\n",
    "    \n",
    "    # les_whole_box = inertia_consistency_box(les_whole_box, max_dist=MAX_DIST_CONSIST, n_consistency=N_CONSISTENCY)\n",
    "    \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    print(\"Writing video\")\n",
    "    for id_les_box, les_box in enumerate(les_whole_box):\n",
    "        print(\"#\" + str(id_les_box+1) + \"/\" + str(len(les_whole_box)), end=\"\\r\")\n",
    "        # write box into image\n",
    "        im = cv2.imread(les_im_path[id_les_box])\n",
    "        # Write frame id in im\n",
    "        cv2.putText(im, str(id_les_box),(10,height), font, 3,(255,255,255),2,cv2.LINE_AA)\n",
    "        for box in les_box:\n",
    "            cv2.rectangle(\n",
    "                im, \n",
    "                (box.x_min, box.y_min), \n",
    "                (box.x_max, box.y_max),\n",
    "                (255, 0, 0),\n",
    "                3\n",
    "            )\n",
    "\n",
    "        # write image to video\n",
    "        video.write(im)\n",
    "    video.release()\n",
    "    print(\"Done\" + \" \"*20)\n",
    "    \n",
    "    return les_whole_box\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pedestrians(data_root, _W, _H, _N):\n",
    "    ''' Return a list of bounding boxes in the format frame, bb_id, x,y,dx,dy '''\n",
    "    \n",
    "    if not os.path.isdir(BACK_SUB_FOLDER_PATH):\n",
    "        # Generate back sub folder\n",
    "        pass\n",
    "    \n",
    "    interface = Interface(data_root, BACK_SUB_FOLDER_PATH)\n",
    "    # print(interface.les_im_path)\n",
    "    \n",
    "    les_wholes_box_id = apply_box_to_video(\n",
    "        les_im_path=interface.les_im_path, \n",
    "        les_im_backsub_path=interface.les_im_backsub_path,\n",
    "        video_name=VIDEO_NAME)\n",
    "    \n",
    "    # Putting in format asked by evaluation function\n",
    "    les_pedestrians = []\n",
    "    for id_frame, frame_boxes in enumerate(les_wholes_box_id):\n",
    "        for id_box, box in enumerate(frame_boxes):\n",
    "            pedestrian = [id_frame+1, id_box+1, box.x_min, box.y_min, box.x_max-box.x_min, box.y_max-box.y_min]\n",
    "            les_pedestrians.append(pedestrian)\n",
    "    \n",
    "    return les_pedestrians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N.Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORING = False\n",
    "if SCORING:\n",
    "    gt = read_gt(gt_path)\n",
    "    check_id = 309\n",
    "    show_annotation(gt, check_id)\n",
    "\n",
    "    print('A perfect score... {}'.format(evaluate_solution(gt, gt, _N)))\n",
    "\n",
    "    # your solution will be tested simply by changing the dataset\n",
    "    # and changing the module, i.e., the following has to work \n",
    "    # with simply using your module \n",
    "    sol = pedestrians(data_root, _W, _H, _N)\n",
    "    print('A great score! {}'.format(evaluate_solution(sol, gt, _N)))\n",
    "    show_annotation(sol, check_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate new pedestrian dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "def read_seq(path):\n",
    "    \n",
    "    def read_header(ifile):\n",
    "        feed = ifile.read(4)\n",
    "        norpix = ifile.read(24)\n",
    "        version = struct.unpack('@i', ifile.read(4))\n",
    "        length = struct.unpack('@i', ifile.read(4))\n",
    "        assert(length != 1024)\n",
    "        descr = ifile.read(512)\n",
    "        params = [struct.unpack('@i', ifile.read(4))[0] for i in range(0,9)]\n",
    "        fps = struct.unpack('@d', ifile.read(8))\n",
    "        # skipping the rest\n",
    "        ifile.read(432)\n",
    "        image_ext = {100:'raw', 102:'jpg',201:'jpg',1:'png',2:'png'}\n",
    "        return {'w':params[0],'h':params[1],\n",
    "                'bdepth':params[2],\n",
    "                'ext':image_ext[params[5]],\n",
    "                'format':params[5],\n",
    "                'size':params[4],\n",
    "                'true_size':params[8],\n",
    "                'num_frames':params[6]}\n",
    "    \n",
    "    ifile = open(path, 'rb')\n",
    "    params = read_header(ifile)\n",
    "    bytes = open(path, 'rb').read()\n",
    "\n",
    "    # this is freaking magic, but it works\n",
    "    extra = 8\n",
    "    s = 1024\n",
    "    seek = [0]*(params['num_frames']+1)\n",
    "    seek[0] = 1024\n",
    "    \n",
    "    images = []\n",
    "    \n",
    "    for i in range(0, params['num_frames']-1):\n",
    "        tmp = struct.unpack_from('@I', bytes[s:s+4])[0]\n",
    "        s = seek[i] + tmp + extra\n",
    "        if i == 0:\n",
    "            val = struct.unpack_from('@B', bytes[s:s+1])[0]\n",
    "            if val != 0:\n",
    "                s -= 4\n",
    "            else:\n",
    "                extra += 8\n",
    "                s += 8\n",
    "        seek[i+1] = s\n",
    "        nbytes = struct.unpack_from('@i', bytes[s:s+4])[0]\n",
    "        I = bytes[s+4:s+nbytes]\n",
    "        \n",
    "        tmp_file = '/tmp/img%d.jpg' % i\n",
    "        open(tmp_file, 'wb+').write(I)\n",
    "        \n",
    "        img = cv2.imread(tmp_file)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        images.append(img)\n",
    "    return images\n",
    "\n",
    "# a = read_seq(\"dataset/set00/V000.seq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disp_image(a[800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from scipy.io import loadmat\n",
    "from collections import defaultdict\n",
    "\n",
    "def read_annot():\n",
    "    all_obj = 0\n",
    "    \n",
    "    for dname in sorted(glob.glob('dataset/annotations/set*')):\n",
    "        data = defaultdict(dict)\n",
    "        set_name = os.path.basename(dname)\n",
    "        data[set_name] = defaultdict(dict)\n",
    "        for anno_fn in sorted(glob.glob('{}/*.vbb'.format(dname))):\n",
    "            vbb = loadmat(anno_fn)\n",
    "            nFrame = int(vbb['A'][0][0][0][0][0])\n",
    "            objLists = vbb['A'][0][0][1][0]\n",
    "            maxObj = int(vbb['A'][0][0][2][0][0])\n",
    "            objInit = vbb['A'][0][0][3][0]\n",
    "            objLbl = [str(v[0]) for v in vbb['A'][0][0][4][0]]\n",
    "            objStr = vbb['A'][0][0][5][0]\n",
    "            objEnd = vbb['A'][0][0][6][0]\n",
    "            objHide = vbb['A'][0][0][7][0]\n",
    "            altered = int(vbb['A'][0][0][8][0][0])\n",
    "            log = vbb['A'][0][0][9][0]\n",
    "            logLen = int(vbb['A'][0][0][10][0][0])\n",
    "\n",
    "            video_name = os.path.splitext(os.path.basename(anno_fn))[0]\n",
    "            data[set_name][video_name]['nFrame'] = nFrame\n",
    "            data[set_name][video_name]['maxObj'] = maxObj\n",
    "            data[set_name][video_name]['log'] = log.tolist()\n",
    "            data[set_name][video_name]['logLen'] = logLen\n",
    "            data[set_name][video_name]['altered'] = altered\n",
    "            data[set_name][video_name]['frames'] = defaultdict(list)\n",
    "\n",
    "            n_obj = 0\n",
    "            for frame_id, obj in enumerate(objLists):\n",
    "                if len(obj) > 0:\n",
    "                    for id, pos, occl, lock, posv in zip(\n",
    "                            obj['id'][0], obj['pos'][0], obj['occl'][0],\n",
    "                            obj['lock'][0], obj['posv'][0]):\n",
    "                        keys = obj.dtype.names\n",
    "                        id = int(id[0][0]) - 1  # MATLAB is 1-origin\n",
    "                        pos = pos[0].tolist()\n",
    "                        occl = int(occl[0][0])\n",
    "                        lock = int(lock[0][0])\n",
    "                        posv = posv[0].tolist()\n",
    "\n",
    "                        datum = dict(zip(keys, [id, pos, occl, lock, posv]))\n",
    "                        datum['lbl'] = str(objLbl[datum['id']])\n",
    "                        datum['str'] = int(objStr[datum['id']])\n",
    "                        datum['end'] = int(objEnd[datum['id']])\n",
    "                        datum['hide'] = int(objHide[datum['id']])\n",
    "                        datum['init'] = int(objInit[datum['id']])\n",
    "                        data[set_name][video_name][\n",
    "                            'frames'][frame_id].append(datum)\n",
    "                        n_obj += 1\n",
    "\n",
    "            print(dname, anno_fn, n_obj)\n",
    "            all_obj += n_obj\n",
    "        \n",
    "        print(data)\n",
    "\n",
    "#     print('Number of objects:', all_obj)\n",
    "#     json.dump(data, open('data/annotations.json', 'w'))\n",
    "#     return data\n",
    "\n",
    "# data = read_annot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
